---
title: "example02_morris"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{example02_morris}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# 1 Setup Directories

## 1.1 Setup root directory
The root directory is where you would like your work to be placed, and the project name is a subdirectory for a specific method, catchment, or whatever the user decides. 

```{r}

rm(list = ls()) # Clears Global Environment!!!
library(pwsMultiObsR)

dir_root <- file.path("/Users/lnorth/Desktop/test_pwsMultiObsR")
project_name <- "example_east"

```

# 2 User Input

## 2.1 Parameters to sample
Out of the 144-ish parameter in PRMS/pywatershed, 52 are found to be appropriate for calibration (one would not want to calibrate HRU elevation, for example). See more details on this in North et al., 2026. You may look in the source code at "R/input_generation/" to find this list of parameter and select the ones you want to sample - an example is provided below. 
   
```{r}

param_names <- c("rad_trncf", "tmax_allsnow", "jh_coef", "tmax_cbh_adj")

```

## 2.2 Add Trial
Use the function pws.search.trial to look at what projects or trials that have been created. Then use the pws.create.trial to create a new sensitivity analysis trial.

```{r}

# Don't remember the projects or trials you already created? Run these!
fm_project_search(dir_root = dir_root)
fm_project_search(dir_root = dir_root, verbose = TRUE)
```

```{r}

?pwsMultiObsR::fm_trial_create

directories <- fm_trial_create(
  dir_root = dir_root,
  project_name = project_name,
  type = "morris",
  param_names = param_names,
  nruns = 20
)

# # If you have already created the trial and want to come back to it, use this:
# 
# directories <- fm_trial_set(
#   dir_root = dir_root,
#   project_name = project_name,
#   trial_number = 2
# )

```

# 3 Run Model
This calls a script that will parallelize the pywatershed runs across half of the available cores on the machine. We are using a 10 year run period between water 2013 and 2023 based on the control.control file.

Common errors include renamed parameter files, control files, cbh_files, etc Note, you must run the default script first to write the climate .nc files. To execute the python script, you may have to go into the shell to unrestrict the ExecutionPolicy (at least on Windows).

The operation will terminate if it detects performance degradation (i.e. computer goes to sleep, memory overload, etc.). Once this limit is reached, you may need to restart the computer to offload any cached memory files if cleanup did not occur. You may then re-run this chunk, the fuction is designed to not overwrite files and start where left off. 

```{r}

?run_batches

run_batches(
  dir_root = dir_root,
  project_name = project_name,
  trial_number = directories$trial_number,
  pwsenvname = "pws"
)

```

# 4 Goodness of Fit
The function calc_gof post-processes the model results using pre-selected "goodness of fit" functions. It writes goodness of fit results in lists named "list_gof_obsType" in the default ouput directory.

```{r}
?calc_gof

gof_results <- calc_gof(
  dir_root = dir_root,
  project_name = project_name,
  trial_number = directories$trial_number,
  start_year = 2013,
  end_year = 2022,
  hru_out_names = "pkwater_equiv",
  seg_out_names = "seg_outflow",
  default = FALSE
)

```

```{r}

# Already calculated GOF and want look at the results again? Use this:

# ?read_output
# 
# gof_results <- read_output(
#   dir_root = dir_root,
#   project_name = project_name,
#   trial_number = directories$trial_number,
#   var_names = c("swe","q"),
#   metrics = c("gof","metric"),
#   list_type = c("gof")
# )

```

# 5 Sensitivity Indices
This function returns a list with a similar structure to the fit statistics lists created above, but it calculates the Morris elementary effects statistics. If NA is specified for both n_boot or sd_level, the function will return baseline statistics with a near instantaneous run time. If bootstrapping, the recommended n_boot is 1000 and sd_level of 1, and it may take 15+ minutes per 10 HRUs and 2500 runs depending on how many datasets are used. 

μ:  Indicates the direction and magnitude of the parameter's effect.
    It is the average of elementary effects, which is essentially change in the
    model response divided by change in parameter value.
μ∗: Indicates the overall magnitude of the parameter's effect without
    considering the direction. Large values imply strong influence.
sigma: Indicates the variability in the parameter's effect. Large values
       suggest interactions or non-linear effects.
η∗: μ∗ normalized by the maximum μ∗ for each dataset
ν∗: μ∗ normalized by the sum of μ∗ for each dataset

```{r}

?sensi_eet

eet_results <- sensi_eet(
  dir_root = dir_root,
  project_name = project_name,
  trial_number = directories$trial_number,
  variables = c("swe","q")
)

```

```{r}

# Already calculated EET and want look at the results again? Use this:

# ?read_output
# 
# eet_results <- read_output(
#   dir_root = dir_root,
#   project_name = project_name,
#   trial_number = directories$trial_number,
#   var_names = c("swe","q"),
#   metrics = c("gof","metric"),
#   list_type = c("eet")
# )

```

# 6 Results

NOTE: THIS FUNCTION IS DESIGNED TO WORK WITH BOOTSRAP RESULTS ONLY.

Lets run through a set of functions to visualize the elementary effects statistics that are calculated above. Following methods from Cuntz et al., 2015, we retrieve η*, which is mu_star normalized by the maximum. 

```{r}

# Read in morris object
morris_design <- readRDS(
  file.path(directories$dir_dynamic_input, "!morris_design.rds"))

# Create list of dataframes to retrieve bootstrap results (lower, mean, upper)
list_eta_dfs <- list()
measures <- c("lower","mean","upper")

for(i in seq_along(measures)){
  
  df_name <- paste0("df_",measures[i])
  
  list_eta_dfs[[df_name]] <- data.frame(
    Q_09112500 = eet_results$eet_list_gof_q$`09112500`$total$nrmse$bootstrap$eta_star[,i],
    Q_09112200 = eet_results$eet_list_gof_q$`09112200`$total$nrmse$bootstrap$eta_star[,i],
    SWE_380 = eet_results$eet_list_gof_swe$CO$`380`$nrmse$bootstrap$eta_star[,i],
    SWE_737 = eet_results$eet_list_gof_swe$CO$`737`$nrmse$bootstrap$eta_star[,i]
  )
  rownames(list_eta_dfs[[df_name]]) <- colnames(morris_design$X)
}

```

Now, lets identify which parameters are sensitive mathematically. This function plots the threshold η* value (in red) to determine if a parameter is sensitive. If above the threshold, it is considered sensitive. Note that the return of this function allows you to view the identified parameters and statistical errors. Some data may have trouble processing due to the small parameter and sample size in this example. 

NOTE: THIS FUNCTION IS DESIGNED TO WORK WITH BOOTSRAP RESULTS ONLY

```{r}

?fit_logis

logis_output <- fit_logis(
  df_mean = list_eta_dfs$df_mean,
  df_lower = list_eta_dfs$df_lower,
  df_upper = list_eta_dfs$df_upper,
  ncol_plot = 2,
  dir_plot = directories$dir_plot,
  start_xo = 0.8,
  start_k = 4
)

```

Plot a heatmap of the sensitivity indices to get a visual sense for what parameters are influential. 

```{r}

?plot_heatmap_eta

param_attributes <- read.csv(
  file.path(directories$dir_dynamic_input, "!param_attributes.csv"))

plot_heatmap_eta(
  df_eta = list_eta_dfs$df_mean,
  type1_params = logis_output$type1_params,
  type2_params = logis_output$type2_params,
  param_attributes = param_attributes,
  dir_plot = directories$dir_plot
)

```

