---
title: "example02_lhs"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{example02_lhs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# 1 Setup Directories

## 1.1 Setup root directory
The root directory is where you would like your work to be placed, and the project name is a subdirectory for a specific method, catchment, or whatever the user decides. 

```{r}

rm(list = ls()) # Clears Global Environment!!!
library(pwsMultiObsR)

dir_root <- file.path("/Users/lnorth/Desktop/test_pwsMultiObsR")
project_name <- "example_east"

```

# 2 User Input

## 2.1 Parameters to sample
Out of the 144-ish parameter in PRMS/pywatershed, 52 are found to be appropriate for calibration (one would not want to calibrate HRU elevation, for example). See more details on this in North et al., 2026. You may look in the source code at "R/input_generation/" to find this list of parameter and select the ones you want to sample - an example is provided below. 
   
```{r}

param_names <- c("rad_trncf", "tmax_allsnow", "jh_coef", "tmax_cbh_adj")

```

## 2.2 Add Trial
Use the function pws.search.trial to look at what projects or trials that have been created. Then use the pws.create.trial to create a new sensitivity analysis trial.

```{r}

# Don't remember the projects or trials you already created? Run these!
fm_project_search(dir_root = dir_root)
fm_project_search(dir_root = dir_root, verbose = TRUE)
```

```{r}

?pwsMultiObsR::fm_trial_create

directories <- fm_trial_create(
  dir_root = dir_root,
  project_name = project_name,
  type = "mmlhs",
  param_names = param_names,
  nruns = 36
)


# # If you have already created the trial and want to come back to it, use this:
# 
# directories <- fm_trial_set(
#   dir_root = dir_root,
#   project_name = project_name,
#   trial_number = 2
# )

```

# 3 Run Model
This calls a script that will parallelize the pywatershed runs across half of the available cores on the machine. We are using a 10 year run period between water 2013 and 2023 based on the control.control file.

Common errors include renamed parameter files, control files, cbh_files, etc Note, you must run the default script first to write the climate .nc files. To execute the python script, you may have to go into the shell to unrestrict the ExecutionPolicy (at least on Windows).

The operation will terminate if it detects performance degradation (i.e. computer goes to sleep, memory overload, etc.). Once this limit is reached, you may need to restart the computer to offload any cached memory files if cleanup did not occur. You may then re-run this chunk, the fuction is designed to not overwrite files and start where left off. 

```{r}

?run_batches

run_batches(
  dir_root = dir_root,
  project_name = project_name,
  trial_number = directories$trial_number,
  pwsenvname = "pws",
  batch_size = 12
)

```

# 4 Goodness of Fit
The function calc_gof post-processes the model results using pre-selected "goodness of fit" functions. It writes goodness of fit results in lists named "list_gof_obsType" in the default ouput directory.

```{r}

?calc_gof

gof_results <- calc_gof(
  dir_root = dir_root,
  project_name = project_name,
  trial_number = directories$trial_number,
  start_year = 2013,
  end_year = 2022,
  hru_out_names = "pkwater_equiv",
  seg_out_names = "seg_outflow",
  default = FALSE
)

```

```{r}

# Already calculated GOF and want look at the results again? Use this:

# ?read_output
# 
# gof_results <- read_output(
#   dir_root = dir_root,
#   project_name = project_name,
#   trial_number = directories$trial_number,
#   var_names = c("swe","q"),
#   metrics = c("gof","metric"),
#   list_type = c("gof")
# )

```

# 5 Results
Lets run through a set of functions to see which simualtions had acceptable performance. First, lets setup our results into a dataframe:

```{r Calculate Sensitivity Indices, warning=TRUE}

df_gof <- data.frame(
  Q_09112500 = gof_results$list_gof_q$`09112500`$total$nrmse,
  Q_09112200 = gof_results$list_gof_q$`09112200`$total$nrmse,
  SWE_380 = gof_results$list_gof_swe$CO$`380`$nrmse,
  SWE_737 = gof_results$list_gof_swe$CO$`737`$nrmse
)

#rownames(list_eta_dfs[[df_name]]) <- colnames(morris_design$X)

```

Now, lets determine which simulations were behavioral. Since we are using NRMSE (expressed as a percentage by hydroGOF::), our acceptable threshold is 100 (shown as 1 in North et al., 2026). It is considered a cost function, since it is an objective function that is to be minimized. 

```{r Calculate Sensitivity Indices, warning=TRUE}

?find_behav

behav <- find_behav(
  df_gof = df_gof,
  gof_thresh = 100,
  cost = TRUE
)

```

Next, we want to find behavioral simulations at the intersection of multiple datasets, instead of them individually. Below, we will define the multi-dataset criteria by calling out the column names of the dataframe created above. 

```{r Calculate Sensitivity Indices, warning=TRUE}

criteria_intersection <- list(
  q_both = colnames(df_gof[c(1,2)]),
  q_snotel_380 = colnames(df_gof[c(2,3)])
)

```

Lastly, we can identify the simulation indices that correspond to the multi-dataset intersections. This function will also output the GOF metric (NRMSE in the example) for the first column (q_col). Since streamflow is often the variable of concern, we may want to investigate how alternative datasets may influence streamflow performance. 

```{r Calculate Sensitivity Indices, warning=TRUE}

?find_behav_intersect

behav_intersect <- find_behav_intersect(
  df_gof = df_gof,
  criteria_names = criteria_intersection,
  idx_behav = behav$idx_behav,
  q_col = 1
)

```


